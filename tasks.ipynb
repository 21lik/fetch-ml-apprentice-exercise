{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using Pytorch for my deep learning module, as I have the most experience with it from coursework and a summer program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, I referenced the textbook Dive into Deep Learning Chapter 11 Section 7 <https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html>, as well as my coursework from COMP SCI 539: Introduction to Artificial Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to implement the transformer architecture, modeled by the figure below:\n",
    "\n",
    "<img src=transformer.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to implement this from scratch, we would need to implement encoder and decoder layers, which would require implementing multi-head attention, feed-forward networks, etc. Fortunately, Pytorch provides a default Tranformer module, which we can use. However, we will still need to provide masked source and target sequences.\n",
    "\n",
    "First, we want to be able to convert a sentence to a fixed-size list of tokens; for convenience, we will use words as tokens and remove punctuation characters. (This means contraction words will each be its own token, with the apostrophe character removed.) We can use padding tokens to attain the specified word count. Next, we will want to convert the tokens into a list of indices, each of which will correspond to a certain word. These indices will be input to an Embedding layer, which will provide the embedded sequences.\n",
    "\n",
    "To do this, we will need to create a dictionary of words, as well as some auxilliary functions and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN_WORD = \"SOS\"  # start of sentence\n",
    "END_TOKEN_WORD = \"EOS\"  # end of sentence\n",
    "PAD_TOKEN_WORD = \"PAD\"  # padding\n",
    "UNKNOWN_TOKEN_WORD = \"UNK\"  # unknnown word # TODO: remove if unused\n",
    "\n",
    "START_TOKEN_IDX = 0  # start of sentence\n",
    "END_TOKEN_IDX = 1  # end of sentence\n",
    "PAD_TOKEN_IDX = 2  # padding\n",
    "UNKNOWN_TOKEN_IDX = 3  # unknnown word # TODO: remove if unused\n",
    "\n",
    "\n",
    "class WordDictionary:\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {\n",
    "            START_TOKEN_WORD: START_TOKEN_IDX,\n",
    "            END_TOKEN_WORD: END_TOKEN_IDX,\n",
    "            PAD_TOKEN_WORD: PAD_TOKEN_IDX,\n",
    "            UNKNOWN_TOKEN_WORD: UNKNOWN_TOKEN_IDX,\n",
    "        }\n",
    "        self.index_to_word = {\n",
    "            START_TOKEN_IDX: START_TOKEN_WORD,\n",
    "            END_TOKEN_IDX: END_TOKEN_WORD,\n",
    "            PAD_TOKEN_IDX: PAD_TOKEN_WORD,\n",
    "            UNKNOWN_TOKEN_IDX: UNKNOWN_TOKEN_WORD,\n",
    "        }\n",
    "        self.word_to_count = {\n",
    "            START_TOKEN_WORD: 0,\n",
    "            END_TOKEN_WORD: 0,\n",
    "            PAD_TOKEN_WORD: 0,\n",
    "            UNKNOWN_TOKEN_WORD: 0,\n",
    "        }\n",
    "        self.n_words = len(self.word_to_index)\n",
    "\n",
    "    def add_word_list(self, sentence: list[str]):\n",
    "        \"\"\"Add a list of words to the dictionary.\"\"\"\n",
    "        for word in sentence:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        \"\"\"Add a word to the dictionary.\"\"\"\n",
    "        if word in self.word_to_index:\n",
    "            self.word_to_count[word] += 1\n",
    "        else:\n",
    "            self.word_to_index[word] = self.n_words\n",
    "            self.word_to_count[word] = 1\n",
    "            self.index_to_word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "\n",
    "def tokenize_and_pad(sentence: str, token_count: int):\n",
    "    \"\"\"Tokenize the sentence and truncate or pad to a list of fixed length.\n",
    "    Punctuation characters are removed.\"\"\"\n",
    "    sentence = (\n",
    "        sentence.upper()\n",
    "        .replace(\".\", \"\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"?\", \"\")\n",
    "        .replace(\"!\", \"\")\n",
    "        .replace(\";\", \"\")\n",
    "        .replace(\":\", \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace('\"', \"\")\n",
    "        .replace(\"-\", \"\")\n",
    "        .replace(\"_\", \"\")\n",
    "    )  # Note: if the input might contain other punctuation marks, include them here\n",
    "    word_list = sentence.split()\n",
    "    word_list.insert(0, START_TOKEN_WORD)\n",
    "    if len(word_list) < token_count:\n",
    "        word_list.append(END_TOKEN_WORD)\n",
    "        word_list.extend([PAD_TOKEN_WORD] * (token_count - len(word_list)))\n",
    "    return word_list[:token_count]\n",
    "\n",
    "\n",
    "def word_list_to_indices(word_list: list[str], word_dict: WordDictionary):\n",
    "    \"\"\"Convert the list of words to a list of indices, updating the word_dict as necessary.\"\"\"\n",
    "    word_dict.add_word_list(word_list)\n",
    "    output = []\n",
    "    for word in word_list:\n",
    "        output.append(word_dict.word_to_index[word])\n",
    "    return output\n",
    "\n",
    "\n",
    "def indices_to_word_list(indices: list[int], word_dict: WordDictionary):\n",
    "    \"\"\"Convert the list of indeces to a list of words.\"\"\"\n",
    "    output = []\n",
    "    for idx in indices:\n",
    "        output.append(word_dict.index_to_word[idx])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a WordDictionary and add a list of sentences we will use. For creating a large language model with real world applications, we would want to add all the English words we will use into the dictionary; however, for the sake of simplicity, we will demonstrate our model using only the words of a few sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = WordDictionary()\n",
    "token_count = 10\n",
    "\n",
    "sentences = [\n",
    "    \"Hello World!\",\n",
    "    \"Roses are red, violets are blue.\",\n",
    "    \"Pineapple belongs on pizza.\",\n",
    "    \"Milk first, then cereal.\",\n",
    "    \"A well-done steak is a steak well-done.\",\n",
    "    \"The ocean is a soup, and soup is a drink.\",\n",
    "    \"Ergo, the ocean is a drink.\",\n",
    "    \"How many times have I told you, Kevin?\",\n",
    "    \"DON'T EAT FOOD OFF THE FLOOR!\",\n",
    "    \"Eyes of mine with fire burn.\",\n",
    "    \"Heart of mine freezing in the blizzard; I am numb.\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = tokenize_and_pad(sentence=sentence, token_count=token_count)\n",
    "    print(tokens)\n",
    "    indices = word_list_to_indices(tokens, word_dict)\n",
    "    print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our Transformer model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix/explain below\n",
    "\n",
    "class SentenceTransformerModel(nn.Transformer):\n",
    "    def __init__(self, sentence_length: int, word_dict: WordDictionary):\n",
    "        super.__init__(self, sentence_length)\n",
    "        self.embedding = nn.Embedding(num_embeddings=word_dict.n_words, embedding_dim=sentence_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        return super.forward(x)\n",
    "\n",
    "\n",
    "\n",
    "transformer = nn.Transformer()\n",
    "\n",
    "transformer(\"Hello World!\", [0, 4, 5, 1, 2, 2, 2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Training Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Training Loop Implementation (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024_wqcc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
