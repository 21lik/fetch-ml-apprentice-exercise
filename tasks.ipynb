{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using Pytorch for my deep learning module, as I have the most experience with it from coursework and a summer program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, I referenced the textbook Dive into Deep Learning Chapter 11 Section 7 <https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html>, as well as my coursework from COMP SCI 539: Introduction to Artificial Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to implement the transformer architecture, modeled by the figure below:\n",
    "\n",
    "<img src=transformer.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll need a module to convert the sentence to a vector with the word and positional encodings. This will be used for both the Encoder and Decoder. We will use these equations for the positional encoding vector:\n",
    "$$PE_{(pos,2i)}=\\sin\\left(pos/10000^{2i/d_{model}}\\right)$$\n",
    "$$PE_{(pos,2i+1)}=\\cos\\left(pos/10000^{2i/d_{model}}\\right)$$\n",
    "These equations were used by Bahdanau et al. in the paper \"Attention Is All You Need\", which introduced the transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encodings(length, depth):\n",
    "    divisors = torch.empty(size=depth / 2, dtype=torch.float64)\n",
    "    for i in range(depth / 2):\n",
    "        divisors[i] = 10000 ** (2 * i / depth)\n",
    "\n",
    "    output = torch.empty(size=(length, depth), dtype=torch.float64)\n",
    "    for pos in range(length):\n",
    "        for i in range(0, depth, 2):\n",
    "            output[pos][i] = np.sin(pos / divisors[i // 2])\n",
    "            output[pos][i] = np.cos(pos / divisors[i // 2])\n",
    "\n",
    "    return output\n",
    "\n",
    "class WordPosEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, sentence_length=10):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.word_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoding = position_encodings(length=sentence_length, depth=d_model)\n",
    "        # TODO: complete, fix signature/parameters?\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.word_embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def forward(self):\n",
    "        # TODO: complete, fix signature/parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: complete, fix signature/parameters?\n",
    "\n",
    "    def forward(self):\n",
    "        # TODO: complete, fix signature/parameters?\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: complete, fix signature/parameters?\n",
    "\n",
    "    def forward(self):\n",
    "        # TODO: complete, fix signature/parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.multi-head_attention = MultiHeadAttention(\n",
    "\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            \n",
    "        )\n",
    "    # TODO: finish/fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.pos_embedding = WordPosEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "        self.encoding_layers = nn.ModuleList([\n",
    "            SentenceEncoderLayer(...)\n",
    "        ])\n",
    "\n",
    "        # embedding\n",
    "        # positional encoding\n",
    "        # N *\n",
    "        ### Multi-Head Attention\n",
    "        #### Add & Norm\n",
    "        ### Feed Forward\n",
    "        #### Add & Norm\n",
    "        # TODO: complete, fix signature/parameters?\n",
    "\n",
    "    def forward(self):\n",
    "        # TODO: complete, fix signature/parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.pos_embedding = \n",
    "        self.encoding_layers = nn.ModuleList([\n",
    "            SentenceEncoderLayer(...)\n",
    "        ])\n",
    "\n",
    "        # embedding\n",
    "        # positional encoding\n",
    "        # N *\n",
    "        ### Multi-Head Attention\n",
    "        #### Add & Norm\n",
    "        ### Feed Forward\n",
    "        #### Add & Norm\n",
    "        # TODO: complete, fix signature/parameters?\n",
    "\n",
    "    def forward(self):\n",
    "        # TODO: complete, fix signature/parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create and explain class\n",
    "\n",
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "\n",
    "        self.encoder = SentenceEncoder(\n",
    "            \n",
    "        )\n",
    "        self.decoder = SentenceDecoder(\n",
    "\n",
    "        )\n",
    "        \n",
    "        # TODO: complete, fix signature/parameters?\n",
    "\n",
    "    def forward(self):\n",
    "        \n",
    "        # TODO: complete, fix signature/parameters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Training Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Training Loop Implementation (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024_wqcc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
